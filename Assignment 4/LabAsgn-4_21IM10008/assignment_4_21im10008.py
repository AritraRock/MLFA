# -*- coding: utf-8 -*-
"""assignment_4_21IM10008.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nETQBcO58TgBQqjTxPJuURjbik2Txe5W

#**Name-Aritra Ray**

#**Roll No.-21IM10008**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

data_csv=pd.read_csv('Iris.csv')
iris_df=pd.DataFrame(data_csv)
iris_df

iris_df['Species'] = iris_df['Species'].replace({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})
iris_df.drop('Id',axis=1,inplace=True)
X=iris_df.drop('Species',axis=1)
Y=iris_df['Species']

class NaiveBayesClassifier:
    def __init__(self, num_bins=3):
        self.num_bins = num_bins
        self.class_probs = None
        self.feature_probs = None

    def _bin_data(self, data):
      binned_data = np.zeros_like(data, dtype=int)
      for i, col in enumerate(data.columns):  # Use enumerate to get both index and column name
        min_val = np.min(data[col])
        max_val = np.max(data[col])
        bins = np.linspace(min_val, max_val, self.num_bins + 1)
        # Adjust bins to include -inf to min_val and max_val to +inf
        bins[0] = float('-inf')
        bins[-1] = float('inf')
        binned_data[:, i] = np.digitize(data[col].to_numpy(), bins) - 1  # Use [:, i] to access column by index
      return binned_data

    def _compute_class_probs(self, labels):
        unique_classes, class_counts = np.unique(labels, return_counts=True)
        self.class_probs = class_counts / np.sum(class_counts)
        return unique_classes

    def _compute_feature_probs(self, data, labels):
      num_classes = len(self.class_probs)
      num_features = data.shape[1]
      self.feature_probs = np.zeros((num_classes, num_features, self.num_bins))

      for i, cls in enumerate(self.classes):
        cls_data = data[labels == cls]
        for j in range(num_features):
            for k in range(self.num_bins):
                count = np.sum(cls_data[:, j] == k)
                self.feature_probs[i, j, k] = (count + 1) / (len(cls_data) + self.num_bins)

    def fit(self, X_train, y_train):
      X_train_binned = self._bin_data(X_train)
      # print(X_train_binned)
      self.classes = self._compute_class_probs(y_train)
      self._compute_feature_probs(X_train_binned, y_train)

    def _predict_instance(self, instance):
        probabilities = []
        for i, cls in enumerate(self.classes):
            prob_cls = self.class_probs[i]
            prob_features = 1.0
            for j, feature in enumerate(instance):
                prob_features *= self.feature_probs[i, j, feature]
            probabilities.append(prob_cls * prob_features)
        return np.argmax(probabilities)

    def predict(self, X_test):
        X_test_binned = self._bin_data(X_test)
        predictions = []
        for instance in X_test_binned:
            predictions.append(self._predict_instance(instance))
        return np.array(predictions)

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X, Y = shuffle(X, Y, random_state=42)

# Splited the shuffled dataset into training and testing sets (80:20 ratio)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

"""#Experiment 1"""

K_values = [2, 3, 5]
accuracy_results = []

for K in K_values:

    nb_classifier = NaiveBayesClassifier(num_bins=K)
    nb_classifier.fit(X_train, y_train)

    y_pred = nb_classifier.predict(X_test)

    # Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_results.append(accuracy)
    print(f"Accuracy for K={K}: {accuracy}")

# Plot Percentage Accuracy vs K
plt.plot(K_values, accuracy_results, marker='o')
plt.xlabel('Number of Bins (K)')
plt.ylabel('Percentage Accuracy')
plt.title('Effect of Varying Number of Bins (K) on Test Data')
plt.grid(True)
plt.show()

# Best value of K
best_K = K_values[np.argmax(accuracy_results)]
best_accuracy = max(accuracy_results)
print(f"Best value of K: {best_K} with accuracy: {best_accuracy}")

"""#Experiment 2"""

def add_noise(data, fraction, noise_std):
    noisy_data = data.copy()
    num_samples = int(len(noisy_data) * fraction)
    noise = np.random.normal(0, noise_std, size=(num_samples, noisy_data.shape[1]))
    noisy_data[:num_samples] += noise
    return noisy_data

noise_fractions = [0.1, 0.4, 0.8, 0.9]
noise_std = 2.0
np.random.seed(42)
# Initialized lists to store accuracy for each noise level
accuracies = []

for fraction in noise_fractions:
    # Noise addition to a fraction of the training data
    noisy_train_data = add_noise(X_train, fraction, noise_std)

    nb_classifier = NaiveBayesClassifier(num_bins=K)
    nb_classifier.fit(noisy_train_data, y_train)

    y_pred = nb_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Accuracies for each noise level
for i, fraction in enumerate(noise_fractions):
    print(f'Accuracy with {fraction*100}% noise: {accuracies[i]}')

# Comparision with noiseless case
print('Accuracy without noise:', best_accuracy)